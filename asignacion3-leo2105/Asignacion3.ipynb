{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Asignación 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "1 .Descarga el conjunto de datos de educación superior de los Estados Unidos de [www.data.gov/education](https://www.data.gov/education/ ) como un archivo `CSV`. Escribe un programa que reporte a  diez instituciones de educación superior (`HEIs`) que son las más cercanas geográficamente a un punto definido por la latitud media y la longitud media de todas las `HEIs` en el conjunto de datos. Calcular las distancias en grados. Usa `numpy` para el almacenamiento y procesamiento de datos tanto como sea posible. Recuerda que la primera fila del archivo CSV contiene columnas de encabezados y que algunos campos o registros enteros del archivo pueden ser inválidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Tu codigo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sin, cos, sqrt, asin, pi\n",
    "\n",
    "R = 6371.0\n",
    "\n",
    "def distance(pos):\n",
    "    diff_lat = pos[1] - mean_lat\n",
    "    diff_long = pos[0] - mean_long\n",
    "    c = pi/180.0\n",
    "    return 2.0 * R * asin(sqrt(sin(c * diff_lat / 2) ** 2) + cos(c * mean_lat) * cos(c * pos[1]) * sin(c * (diff_long) / 2) ** 2)\n",
    "\n",
    "data = pd.read_csv('Dataset_1/data.csv', encoding = \"ISO-8859-1\")\n",
    "coord = np.array(data[['LONGITUD', 'LATITUDE']])\n",
    "\n",
    "# Se calcula la latitud y longitud media de la data\n",
    "mean_long = np.mean(coord[:, 0])\n",
    "mean_lat = np.mean(coord[:, 1])\n",
    "\n",
    "# Aplicar la funcion distance sobre el np.array coord\n",
    "dis = np.apply_along_axis(distance, 1, coord)\n",
    "\n",
    "# Se calcula la posicion de los 10 primera valores ordenados\n",
    "indices = np.argpartition(dis, 10, axis=0)[:10]\n",
    "\n",
    "# Las 10 instituciones mas cercanas a la media\n",
    "\n",
    "print(\"Los 10 institutos de educ sup mas cercanos a la institucion media : \")\n",
    "print(\"-------------------------------------------------------\")\n",
    "for i in indices:\n",
    "    print(data['INSTNM'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "2 .Wikipedia tiene muchos datos sobre diversos aspectos demográficos, incluyendo la \n",
    "[Lista de paises por consumo de alcohol per capita](https://en.wikipedia.org/wiki/List_of_countries_by_alcohol_consumption_per_capita) y la [Lista de paises por  GDP (PPP) per capita](https://en.wikipedia.org/wiki/List_of_countries_by_GDP_per_capita). \n",
    "\n",
    "Escribe un programa que utilice estos datos y realize tabulación cruzada entre el nivel del GDP (por  encima del promedio vs. por debajo del promedio) vs. nivel de consumo de alcohol (por encima del promedio vs. por debajo del promedio). En base a la tabla realizada, ¿las dos medidas están correlacionadas?."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Tu codigo\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import urllib.request\n",
    "\n",
    "wiki = 'https://en.wikipedia.org/wiki/List_of_countries_by_alcohol_consumption_per_capita'\n",
    "\n",
    "with urllib.request.urlopen(wiki) as url:\n",
    "    s = url.read()\n",
    "    \n",
    "soup = BeautifulSoup(s)\n",
    "\n",
    "rank = ''\n",
    "country = ''\n",
    "total = ''\n",
    "\n",
    "table = soup.find('table', {'class': 'wikitable sortable'})\n",
    "\n",
    "f = open('Dataset_2/alcoholpercapita.csv', 'w')\n",
    "flag = False\n",
    "\n",
    "for row in table.findAll('tr'):\n",
    "    cells = row.findAll('td')\n",
    "    if len(cells) == 10:\n",
    "        rank = cells[0].find(text=True)\n",
    "        country = cells[1].findAll(text=True)\n",
    "        total = cells[2].find(text=True)\n",
    "        \n",
    "    if (not flag):\n",
    "        write_to_file = 'RANK-OH,PAIS,TOTAL\\n'\n",
    "    else:\n",
    "        write_to_file = str(rank) + ',' + str(country[1:]).strip('[]') + ',' + str(total) + '\\n'\n",
    "    flag = True\n",
    "    f.write(write_to_file)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Scrappeando List_of_countries_by_GDP_(PPP)_per_capita\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import urllib.request\n",
    " \n",
    "wiki = 'https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(PPP)_per_capita'\n",
    "\n",
    "with urllib.request.urlopen(wiki) as url:  \n",
    "    s = url.read()\n",
    "\n",
    "soup = BeautifulSoup(s)\n",
    "\n",
    "rank = ''\n",
    "country = ''\n",
    "total = ''\n",
    "\n",
    "table = soup.find('table', { 'class': 'wikitable sortable'})\n",
    "\n",
    "f = open('Dataset_2/gdppercapita.csv', 'w')\n",
    "flag = False\n",
    "\n",
    "for row in table.findAll('tr'):\n",
    "    cells = row.findAll('td')\n",
    "    if len(cells) == 3:\n",
    "        rank = cells[0].find(text=True)\n",
    "        country = str(cells[1].findAll(text=True)[1:]).replace(',', ' ')\n",
    "        total = cells[2].find(text=True).replace(',', '.')\n",
    "    if (not flag):\n",
    "        write_to_file = 'RANK-GDP,PAIS,INT\\n'\n",
    "    else:\n",
    "        write_to_file = str(rank) + ',' + str(country[1:]).strip('[]') + ',' + str(total) + '\\n'\n",
    "    flag = True\n",
    "    f.write(write_to_file)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('Dataset_2/alcoholpercapita.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data2 = pd.read_csv('Dataset_2/gdppercapita.csv')\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = pd.merge(data, data2, on='PAIS')\n",
    "pd.crosstab(result.INT, result.TOTAL, margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "3 .Combina los datos históricos del consumo de alcohol, con los datos meteorológicos históricos por estado. Utiliza la tabulación cruzada para estimar si los hábitos de consumo de alcohol posiblemente están correlacionados con las temperaturas locales promedio y las precipitaciones totales. En otras palabras, ¿la gente bebe más cuando llueve ?. El sitio web del [Centro Nacional de Datos Climáticos](https://www.ncdc.noaa.gov/cdo-web/)  es un buen punto de partida, para lss búsqueda de datos meteorológicos históricos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Se escogera la cantidad de precipitacion en cada estado en el anho 2016\n",
    "# Todo la data se consiguio del link propuesto.\n",
    "# Tu codigo\n",
    "import pandas as pd\n",
    "drunk_data = pd.read_csv('drunk_data.csv',error_bad_lines=False, header=None, names=['Country', 'Drunk Driving Deaths' ])\n",
    "\n",
    "Data={}\n",
    "a = ''\n",
    "\n",
    "# \"Data\" es el diccionario de datos iniciales.\n",
    "for s in drunk_data['Country']:\n",
    "    path = 'Datasets/' + s + '.csv'\n",
    "    Data[s] = pd.read_csv(path, header=None, names=['DAY','CENTERLAT','CENTERLON','data','TEMP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# \"Data_mod\" ha sido modificada\n",
    "# Cambiaremos cada fecha por su estado correspondiente para usar\n",
    "# tabulacion cruzada.\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "Data_mod_1 = {}\n",
    "Data_mod_2 = {}\n",
    "for s in drunk_data['Country']:\n",
    "    Data_mod_1[s] = Data[s][:]['DAY'].copy()\n",
    "    Data_mod_1[s][:] = s # Cambiar fecha por estados.\n",
    "\n",
    "for s in drunk_data['Country']:\n",
    "    Data_mod_2[s] = Data[s][:]['TEMP'].copy()    \n",
    "\n",
    "countries = []\n",
    "temperatures = []\n",
    "\n",
    "for s in Data_mod_1:\n",
    "    for x in Data_mod_1[s] :\n",
    "        countries.append(x)\n",
    "    for x in Data_mod_2[s]:\n",
    "        temperatures.append(x)\n",
    "\n",
    "a = np.array(countries)\n",
    "b = np.array(a)\n",
    "m = crosstab(a, b, rownames=['a'], colnames=['b'])\n",
    "\n",
    "# FRECUENCIA DE PRECIPITACIONES Y TEMPERATURA PROMEDIO\n",
    "frecuencias_prec = []\n",
    "temperatura_prom = []\n",
    "for s in drunk_data['Country']:\n",
    "    if s == 'Nevada' or s == 'Oregon':\n",
    "        frecuencias_prec.append([s, 0]) # No hay precipitacion registradas en esos estados\n",
    "        temperatura_prom.append([Data_mod_2[s].mean()])\n",
    "    else:\n",
    "        frecuencias_prec.append([s, m[s][s]])\n",
    "        temperatura_prom.append([Data_mod_2[s].mean()])\n",
    "\n",
    "        temperatura_prom = [[.0] if math.isnan(x[0]) else x for x in temperatura_prom] # Eliminar valores nan\n",
    "\n",
    "# FRECUENCIA DE MUERTE DE CONDUCTORES EN ESTADO DE EBRIEDAD\n",
    "drunk_state = []\n",
    "for s in drunk_data['Drunk Driving Deaths']:\n",
    "    drunk_state.append([s])\n",
    "drunk_state\n",
    "\n",
    "tabla1 = pd.DataFrame(frecuencias_prec)\n",
    "tabla2 = pd.DataFrame(temperatura_prom)\n",
    "tabla3 = pd.DataFrame(drunk_state)\n",
    "\n",
    "tabla = pd.DataFrame(pd.concat([tabla1, tabla2, tabla3], ignore_index=True, axis=1))\n",
    "tabla \n",
    "\n",
    "result = tabla.sort_values([1], ascending=[1])\n",
    "result\n",
    "\n",
    "# PODEMOS VER UN CUADRO DONDE \n",
    "# COLUMNA 0 : ESTADOS\n",
    "# COLUMNA 1 : CANTIDAD DE PRECIPITACIONES REGISTRADAS EN UN AÑO\n",
    "# COLUMNA 2 : TEMPERATURA PROMEDIO EN UN AÑO\n",
    "# COLUMNA 3 : CANTIDAD DE MUERTES POR CONDUCTORES EN ESTADO DE EBRIEDAD\n",
    "\n",
    "# SE ORDENO EL CUADRO SEGUN LA CANTIDAD DE PRECIPITACIONES.\n",
    "# VEMOS QUE LA CANTIDAD DE MUERTES POR ACCIDENTES DE TRANSITO AUMENTA DE 3 CIFRAS A 4 CIFRAS.\n",
    "\n",
    "# NO PODEMOS DESCARTAR QUE \"A MAYOR CANTIDAD DE LLUVIAS, LA GENTE BEBE MAS\", TRATANDO DE MODIFICAR\n",
    "# LA PREGUNTA INICIAL.\n",
    "\n",
    "# POR LO TANTO, PODRIA EXISTIR UN CORRELACION POSITIVA ENTRE LLUVIAS \n",
    "# Y PERSONAS QUE BEBEN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "4 .Escriba un programa que reporte algunas medidas estadísticas básicas de los valores de cierre del índice `S&P500`: la media, la desviación estándar, la asimetría y la correlación entre los valores de cierre y el volumen de comercio en el siglo XXI. ¿Es fiable la correlación?. Puedes descargar los precios históricos desde  [Yahoo! Finanzas](https://finance.yahoo.com/quote/%5EGSPC/history?ltr=1). Recuerda que el siglo XXI comenzó el 1 de enero de 2001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Tu codigo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('Dataset_4/^GSPC.csv', encoding = \"ISO-8859-1\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CALCULO DE MEDIA Y LA DESVIACION ESTANDAR\n",
    "\n",
    "print(\"LA MEDIA ES: \" + str(data['Close'].mean()))\n",
    "print(\"LA DESVIACION ESTANDAR ES: \" + str(data['Close'].std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# GRAFICA DE LA \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.array(range(1,4141))\n",
    "y = np.array(data['Close'])\n",
    "plt.plot(x, y)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# COEFICIENTE DE ASIMETRIA DE FISHER\n",
    "a3 = np.sum(y**3) / y.size\n",
    "a1 = np.sum(y) / y.size\n",
    "desv = data['Close'].std()\n",
    "\n",
    "u3 = a3 - 3*a1*(desv**2) + 2*(a1**3)\n",
    "\n",
    "asi_fisher = u3 / (desv**3)\n",
    "print(str(asi_fisher) + \" es mayor que 0\")\n",
    "\n",
    "# CONCLUIMOS QUE LA ASIMETRIA ES POSITIVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CALCULO DE CORRELACION\n",
    "vector1 = data['Close']\n",
    "vector2 = data['Volume']\n",
    "print(stats.pearsonr(vector1, vector2)[0])\n",
    "\n",
    "# PRESENTA UNA CORRELACION POSITIVA PERO NO ES FIABLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "5 . Desarrolla  un programa que predice si una ciudad tiene un sistema de metro. Su programa puede necesitar considerar la población, la densidad de población, el tamaño del presupuesto, las condiciones climáticas, los niveles del impuesto sobre la renta y otras variables. Algunos de ellos pueden estar fácilmente disponibles en línea, otros no. Utiliza la regresión logística y un  bosque de decisión aleatorio y escoge  el método que mejor se acomode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Tu codigo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "6 .Escribe un programa que agrupa enormes sitios de redes sociales en línea por el número de usuarios registrados y la [clasificación de página global de Alexa](https://en.wikipedia.org/wiki/List_of_social_networking_websites). Debido a que los rangos y los tamaños del sitio varían en un amplio rango, utiliza la escala logarítmica tanto para la agrupación como para la presentación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tu codigo\n",
    "# Scrappeando List_of_countries_by_GDP_(PPP)_per_capita\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import urllib.request\n",
    " \n",
    "wiki = 'https://en.wikipedia.org/wiki/List_of_social_networking_websites'\n",
    "\n",
    "with urllib.request.urlopen(wiki) as url:  \n",
    "    s = url.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "soup = BeautifulSoup(s)\n",
    "\n",
    "table = soup.find('table', { 'class': 'wikitable sortable'})\n",
    "f = open('Dataset_6/socialnetworkingwebsites.csv', 'w')\n",
    "write_to_file = 'Name,Registered users,Alexa Page Ranking\\n'\n",
    "f.write(write_to_file)\n",
    "for row in table.findAll('tr'):\n",
    "    cells = row.findAll('td')\n",
    "\n",
    "    if len(cells) == 5:\n",
    "        \n",
    "        aux1 = cells[2].findAll(text=True) \n",
    "        aux2 = cells[4].findAll(text=True)\n",
    "        \n",
    "        if len(aux1) > 0 and len(aux2) > 0 and aux2[0] != 'NA':\n",
    "            \n",
    "            aux = row.findAll('th')[0].findAll(text=True)\n",
    "            aux = aux[0].replace(',',' ')\n",
    "            \n",
    "            users = aux1[1].split(' ')[0].replace(',','')\n",
    "            \n",
    "            rank = aux2[1].replace(',','')\n",
    "            \n",
    "            write_to_file = str(aux) + ',' + str(users) + ',' + str(rank) + '\\n'\n",
    "            f.write(write_to_file)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data1 = pd.read_csv('Dataset_6/socialnetworkingwebsites.csv')\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as pl\n",
    "import math\n",
    "from sklearn import neighbors\n",
    "\n",
    "data2 = np.array(data1).copy()\n",
    "\n",
    "X = np.array(data2[:, 1], dtype='double')\n",
    "Y = np.array(data2[:, 2], dtype='double')\n",
    "\n",
    "X = np.log(X)\n",
    "Y = np.log(Y)\n",
    "\n",
    "pl.scatter(X, Y, c='b')\n",
    "pl.xlabel('Registered users')\n",
    "pl.ylabel('Alexa Page Ranking')\n",
    "\n",
    "pl.xlim(X.min(), X.max())\n",
    "pl.ylim(Y.min(), Y.max())\n",
    "\n",
    "\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as pl\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "data2 = np.array(data1).copy()\n",
    "\n",
    "X = np.array(data2[:, 1], dtype='double')\n",
    "Y = np.array(data2[:, 2], dtype='double')\n",
    "\n",
    "X = np.log(X)\n",
    "Y = np.log(Y)\n",
    "\n",
    "# ----------\n",
    "# K - Means\n",
    "\n",
    "\n",
    "datos = np.concatenate((X.reshape((X.shape[0], 1)).T, Y.reshape((Y.shape[0], 1)).T), axis=0).T\n",
    "\n",
    "model = KMeans(n_clusters=2)\n",
    "model.fit(datos)\n",
    "model.labels_\n",
    "\n",
    "# Colorear cada cluster\n",
    "colormap = np.array(['red', 'black'])\n",
    "\n",
    "# ----------\n",
    "\n",
    "pl.scatter(X, Y, c=colormap[model.labels_])\n",
    "pl.xlabel('Registered users')\n",
    "pl.ylabel('Alexa Page Ranking')\n",
    "\n",
    "pl.xlim(X.min(), X.max())\n",
    "pl.ylim(Y.min(), Y.max())\n",
    "\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Una base de datos NoSQL es una colección no volátil de objetos, a menudo conocidos como documentos, con atributos. Se han desarrollado muchas implementaciones diferentes de como almacenar  documentos.  Uno de los más importantes es [MongoDB](https://www.mongodb.com/) que es una base de datos no relacional.\n",
    "\n",
    "Un servidor MongoDB puede soportar varias bases de datos no relacionadas. Una base de datos consiste en una o más colecciones de documentos. Todos los documentos de una colección tienen identificadores únicos. \n",
    " \n",
    "Un cliente de Python para  MongoDB se implementa en el módulo `pymongo` como una instancia de la clase `MongoClient`. Se puede crear un cliente sin parámetros (funciona para una instalación de servidor local típica), con el nombre del host y el número de puerto del servidor como parámetros o con el URI (Uniform Resource Identifier) del servidor como parámetro:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "7 .Ejecuta las siguientes lineas:\n",
    "\n",
    "```\n",
    "import pymongo as mongo\n",
    "cliente1 = mongo.MongoClient()\n",
    "cliente2 = mongo.MongoClient(\"localhost\", 27017)\n",
    "cliente3 = mongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymongo as mongo\n",
    "# MongoClient create una conexion. Si no se especifica argumentos,\n",
    "# se especifica como \"localhost\", 27017.\n",
    "cliente1 = mongo.MongoClient() \n",
    "\n",
    "# Por argumentos se especifica la interfaz donde se va a correr,\n",
    "# ademas del puerto.\n",
    "cliente2 = mongo.MongoClient(\"localhost\", 27017)\n",
    "\n",
    "# Se especifica el MongoDB URI completo para definir la conexion.\n",
    "cliente3 = mongo.MongoClient(\"mongodb://localhost:27017/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "8 . Explica las siguientes lineas\n",
    "\n",
    "```\n",
    "# Se asigna la base de datos llamada dsdb a la variable local db.\n",
    "db = cliente1.dsdb\n",
    "# Tambien se puede acceder como si fuera un diccionario.\n",
    "db = cliente1[\"dsdb\"]\n",
    "# Se puede asignar a la variable persona la coleccion db.persona de dos # formas\n",
    "persona = db.persona\n",
    "persona = db[\"persona\"]\n",
    "\n",
    "# Creamos 2 diccionarios llamados persona1 y persona2.\n",
    "persona1 = {\"empname\" : \"Scikit Learn\", \"dob\" : \"1957-12-24\"}\n",
    "persona2 = {\"_id\" : \"XVT162\", \"empname\" : \"Numpy\", \"dob\" : \"1964-05-16\"}\n",
    "# Inserta el diccionario  persona1 en la coleccion persona. Si no \n",
    "# existiera la crearia.\n",
    "# Con la operacion inserted_id retorna el id del diccionario ingresado.\n",
    "persona_id1 = persona.insert_one(persona1).inserted_id\n",
    "persona1\n",
    "\n",
    "# Se obtiene el id del documento persona2 ingresado.\n",
    "persona_id2 = persona.insert_one(persona2).inserted_id\n",
    "\n",
    "# Crea un iterable con 2 documentos.\n",
    "persons = [{\"empname\" : \"Scipy\", \"dob\" : \"1809-02-12\"},\n",
    "{\"empname\" : \"MongoDb\"}]\n",
    "# Inserta un iterable de documentos declarado como persons.\n",
    "resultados = persona.insert_many(persons)\n",
    "# Devuelve el id de los documentos insertados\n",
    "resultados.inserted_ids\n",
    "\n",
    "# Se asigna a la variable todos a todos los documentos incluidos en la \n",
    "# coleccion persona.\n",
    "todos = persona.find()\n",
    "# Lista los documentos.\n",
    "list(todos)\n",
    "\n",
    "# Nos retorna una lista con todas las coincidencias.\n",
    "list(persona.find({\"dob\" : \"1957-12-24\"}))\n",
    "# Encuentro el primer elemento ingresado, ya que no se le asigno ningun # parametro.\n",
    "persona.find_one()\n",
    "\n",
    "# Devuelve la primera coincidencia al argumento pasado.\n",
    "persona.find_one({\"empname\" : \"Scipy\"})\n",
    "persona.find_one({\"_id\" : \"XVT162\"})\n",
    "\n",
    "# Muestra la cantidad de documentos contenidos en la coleccion.\n",
    "persona.count()\n",
    "\n",
    "# Muestra la cantidad de coincidencias con el argumento pasado.\n",
    "persona.find({\"dob\": \"1957-12-24\"}).count()\n",
    "# De la lista de documentos en la coleccion persona se ordena de acuerdo\n",
    "# al string \"dob\"\n",
    "persona.find().sort(\"dob\")\n",
    "\n",
    "# Remueve todos los documentos que coincidan con {\"dob\" : \"1957-12-24\"}.\n",
    "resultados = persona.delete_many({\"dob\" : \"1957-12-24\"})\n",
    "# Muestra la cantidad de documentos removidos.\n",
    "resultados.deleted_count\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
